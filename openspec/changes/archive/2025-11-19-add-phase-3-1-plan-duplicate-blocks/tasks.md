## 1. Implementation
- [x] 1.1 Build the duplicate_block planner module.
  - [x] 1.1.1 Add `ai_clean/planners/__init__.py` (if missing) and `ai_clean/planners/duplicate.py` exporting `plan_duplicate_block(finding: Finding, *, occurrence_batch: list[dict[str, int]]) -> CleanupPlan`.
  - [x] 1.1.2 Inside `plan_duplicate_block`, map each `occurrence_batch` entry (`{"path": ..., "start_line": ..., "end_line": ...}`) into `FindingLocation` objects saved on the resulting plan.
  - [x] 1.1.3 Populate each `CleanupPlan` with deterministic `title`/`intent` strings (e.g., `"Extract helper for {symbol or path}"`) summarizing the helper extraction for that batch only.
- [x] 1.2 Encode the exact plan steps, constraints, and default tests.
  - [x] 1.2.1 Add helper `_build_duplicate_steps(helper_name: str, helper_module: str, locations: list[FindingLocation]) -> list[str]` returning a fixed 3-step sequence: choose helper destination, implement helper, replace each duplicated span.
  - [x] 1.2.2 Within `plan_duplicate_block`, append constraints covering "Do not change public APIs", "Keep helper signature identical", and "Touch only the listed files".
  - [x] 1.2.3 Populate `tests_to_run` with `["Run existing unit test suite (pytest)"]` so every plan has deterministic validation guidance.
- [x] 1.3 Provide splitting helpers for large findings.
  - [x] 1.3.1 Implement `_chunk_occurrences(occurrences: list[dict[str, int]], limit: int = 3) -> list[list[dict[str, int]]]` to produce the `occurrence_batch` inputs consumed by `plan_duplicate_block`.
  - [x] 1.3.2 Within `plan_duplicate_block`, store metadata derived from the provided batch only (e.g., `metadata["batch_size"] = len(occurrence_batch)` and `metadata["occurrence_paths"]`) so downstream tooling can trace which duplicates were covered.
