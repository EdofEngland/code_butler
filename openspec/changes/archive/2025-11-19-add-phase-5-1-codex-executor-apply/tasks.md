## 1. Implementation
- [x] 1.1 Implement apply_spec to call the configured apply command for a spec file.
  - [x] 1.1.1 Extend `ai_clean/config.py` by adding an `apply_command: list[str]` field to `ExecutorConfig`, parse `[executor].apply_command` from `ai-clean.toml`, and raise `ValueError` when the list is missing, empty, or lacks a `{spec_path}` placeholder.
  - [x] 1.1.2 Update `ai-clean.toml` with a representative `apply_command` entry (e.g., `["bash", "-lc", "openspec apply {spec_path}"]`) plus an inline comment explaining that `{spec_path}` is replaced automatically.
  - [x] 1.1.3 Create `ai_clean/executors/__init__.py` and `ai_clean/executors/codex.py`, exporting a `CodexExecutor` class that stores the configured command tokens.
  - [x] 1.1.4 Inside `CodexExecutor.apply_spec`, resolve the provided `spec_path` to an absolute `Path`, verify the file exists, and render the configured command by replacing `{spec_path}` in every token with that absolute path (raising a clear error if the placeholder is missing).
- [x] 1.2 Capture exit code, stdout, and stderr into ExecutionResult.
  - [x] 1.2.1 Use `subprocess.run(command, capture_output=True, text=True, check=False)` so failures still produce logs without throwing.
  - [x] 1.2.2 Populate `ExecutionResult` with `spec_id` derived from `Path(spec_path).stem`, `success` determined by `process.returncode == 0`, `stdout`/`stderr` set to stripped strings, and `tests_passed=None`.
  - [x] 1.2.3 Store the invoked command tokens and numeric exit code under `execution_result.metadata["apply"]` for later troubleshooting.
- [x] 1.3 Provide metadata scaffolding so tests integration (Phase 5.2) has clear handoff points.
  - [x] 1.3.1 When the apply command fails, set `metadata["tests"] = {"skipped": True, "reason": "apply_failed"}` so the follow-up change can short-circuit cleanly.
  - [x] 1.3.2 When tests are not configured, expose `{"skipped": True, "reason": "tests_disabled"}` to communicate why `tests_passed` remains `None`.
